{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"e9549a8f-6384-4850-b05e-4dd6fdcd51d4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models have gained significant attention in the field of natural language processing (NLP) and artificial intelligence (AI) in recent years, and for good reason. Here are some of the key importance of fast language models:\n",
      "\n",
      "1. **Faster Response Times**: Fast language models can process and respond to user input in real-time, making them ideal for applications that require instant feedback, such as chatbots, virtual assistants, and language translation tools.\n",
      "2. **Improved User Experience**: By providing quick and accurate responses, fast language models can significantly improve the user experience, especially in applications that require rapid processing, such as live chat support and customer service.\n",
      "3. **Enhanced Performance in Real-Time Applications**: Fast language models are particularly effective in real-time applications, such as:\n",
      "\t* Speech-to-text systems: Can transcribe speech in real-time, enabling applications like speech recognition and voice assistants.\n",
      "\t* Text classification: Can classify text in real-time, making it useful for applications like spam detection and sentiment analysis.\n",
      "\t* Machine translation: Can translate text in real-time, enabling seamless communication across languages.\n",
      "4. **Scalability**: Fast language models can handle large volumes of input data, making them suitable for applications that require processing large amounts of text, such as:\n",
      "\t* Information retrieval: Can efficiently retrieve relevant information from a large corpus of text.\n",
      "\t* Sentiment analysis: Can analyze large volumes of text to identify sentiment patterns and trends.\n",
      "5. **Cost-Effective**: Fast language models can reduce computational costs by minimizing the number of iterations and computations required to process input data.\n",
      "6. **Advancements in NLP Research**: Fast language models have enabled researchers to explore new directions in NLP research, such as:\n",
      "\t* Adversarial attacks and defenses\n",
      "\t* Explainability and interpretability\n",
      "\t* Attention mechanisms and contextualized language understanding\n",
      "7. **Enhanced Capabilities for Edge Computing**: Fast language models can be deployed on edge devices, enabling real-time processing and analysis of data at the edge of the network, reducing latency and improving overall performance.\n",
      "8. **Improved Model Personalization**: Fast language models can adapt to individual users' preferences and behaviors, enabling personalized language understanding and generation.\n",
      "9. **Multitasking and Multimodal Integration**: Fast language models can be used for multitasking and multimodal integration, enabling applications like:\n",
      "\t* Visual question answering\n",
      "\t* Conversational games\n",
      "\t* Human-robot interaction\n",
      "10. **Future-Proofing Language Models**: Fast language models can be used as a foundation for more advanced language models, enabling the development of more sophisticated and capable language understanding and generation systems.\n",
      "\n",
      "In summary, fast language models have revolutionized the field of NLP and AI by providing faster, more accurate, and more scalable language processing capabilities. These models have far-reaching implications for various industries and applications, making them an essential component of modern language processing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(\"quickstart\")  # Deletes the existing index\n",
    "\n",
    "index_name = \"quickstart\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=1024, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'values': [0.04925537109375, -0.01313018798828125, ..., -0.01971435546875, -0.01107025146484375]}\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\"id\": \"vec1\", \"text\": \"Apple is a popular fruit known for its sweetness and crisp texture.\"},\n",
    "    {\"id\": \"vec2\", \"text\": \"The tech company Apple is known for its innovative products like the iPhone.\"},\n",
    "    {\"id\": \"vec3\", \"text\": \"Many people enjoy eating apples as a healthy snack.\"},\n",
    "    {\"id\": \"vec4\", \"text\": \"Apple Inc. has revolutionized the tech industry with its sleek designs and user-friendly interfaces.\"},\n",
    "    {\"id\": \"vec5\", \"text\": \"An apple a day keeps the doctor away, as the saying goes.\"},\n",
    "    {\"id\": \"vec6\", \"text\": \"Apple Computer Company was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne as a partnership.\"}\n",
    "]\n",
    "\n",
    "embeddings = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[d['text'] for d in data],\n",
    "    parameters={\"input_type\": \"passage\", \"truncate\": \"END\"}\n",
    ")\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for the index to be ready\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vectors = []\n",
    "for d, e in zip(data, embeddings):\n",
    "    vectors.append({\n",
    "        \"id\": d['id'],\n",
    "        \"values\": e['values'],\n",
    "        \"metadata\": {'text': d['text']}\n",
    "    })\n",
    "\n",
    "index.upsert(\n",
    "    vectors=vectors,\n",
    "    namespace=\"ns1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give me the detailed code with steps for creating a new index in pinecone\"\n",
    "\n",
    "embedding = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[query],\n",
    "    parameters={\n",
    "        \"input_type\": \"query\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'vec4',\n",
      "              'metadata': {'text': 'Apple Inc. has revolutionized the tech '\n",
      "                                   'industry with its sleek designs and '\n",
      "                                   'user-friendly interfaces.'},\n",
      "              'score': 0.724469483,\n",
      "              'values': []},\n",
      "             {'id': 'vec6',\n",
      "              'metadata': {'text': 'Apple Computer Company was founded on '\n",
      "                                   'April 1, 1976, by Steve Jobs, Steve '\n",
      "                                   'Wozniak, and Ronald Wayne as a '\n",
      "                                   'partnership.'},\n",
      "              'score': 0.713796139,\n",
      "              'values': []},\n",
      "             {'id': 'vec1',\n",
      "              'metadata': {'text': 'Apple is a popular fruit known for its '\n",
      "                                   'sweetness and crisp texture.'},\n",
      "              'score': 0.705310583,\n",
      "              'values': []}],\n",
      " 'namespace': 'ns1',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "results = index.query(\n",
    "    namespace=\"ns1\",\n",
    "    vector=embedding[0].values,\n",
    "    top_k=3,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. has revolutionized the tech industry with its sleek designs and user-friendly interfaces.\n",
      "Apple Computer Company was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne as a partnership.\n",
      "Apple is a popular fruit known for its sweetness and crisp texture.\n"
     ]
    }
   ],
   "source": [
    "for result in results['matches']:\n",
    "    print(result['metadata']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 23:06:41.356 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.364 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2024-10-17 23:06:41.364 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.365 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.365 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.474 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\ProgramData\\miniconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-10-17 23:06:41.476 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.477 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.479 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.482 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.483 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.483 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.488 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 23:06:41.488 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import streamlit as st\n",
    "import base64\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "st.session_state.api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "# Only show the API key input if the key is not already set\n",
    "if not st.session_state.api_key:\n",
    "    # Ask the user's API key if it doesn't exist\n",
    "    api_key = st.text_input(\"Enter API Key\", type=\"password\")\n",
    "    \n",
    "    # Store the API key in the session state once provided\n",
    "    if api_key:\n",
    "        st.session_state.api_key = api_key\n",
    "        st.rerun()  # Refresh the app once the key is entered to remove the input field\n",
    "else:\n",
    "    # If the API key exists, show the chat app\n",
    "    st.title(\"Chat App\")\n",
    "\n",
    "    # Initialize the chat message list in session state if it doesn't exist\n",
    "    if \"chat_messages\" not in st.session_state:\n",
    "        st.session_state.chat_messages = []\n",
    "\n",
    "    # Display previous chat messages\n",
    "    for messages in st.session_state.chat_messages:\n",
    "        if messages[\"role\"] in [\"user\", \"assistant\"]:\n",
    "            with st.chat_message(messages[\"role\"]):\n",
    "                st.markdown(messages[\"content\"])\n",
    "    \n",
    "    # Define a function to simulate chat interaction (you would replace this with an actual API call)\n",
    "    def get_chat():\n",
    "        embedding = pc.inference.embed(\n",
    "            model=\"multilingual-e5-large\",\n",
    "            inputs=[st.session_state.chat_messages[-1][\"content\"]],\n",
    "            parameters={\n",
    "                \"input_type\": \"query\"\n",
    "            }\n",
    "        )\n",
    "        results = index.query(\n",
    "            namespace=\"ns1\",\n",
    "            vector=embedding[0].values,\n",
    "            top_k=3,\n",
    "            include_values=False,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        context = \"\"\n",
    "        for result in results['matches']:\n",
    "            context += result['metadata']['text'] + \"\\n\"\n",
    "            \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=st.session_state.chat_messages,\n",
    "            model=\"llama3-8b-8192\",\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "\n",
    "    # Handle user input\n",
    "    if prompt := st.chat_input(\"Try asking the bot what it can do, or thank it for its help!\"):\n",
    "        # Display user message\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "        st.session_state.chat_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        # Get the assistant's response (in this case, it's just echoing the prompt)\n",
    "        with st.spinner(\"Getting responses...\"):\n",
    "            response = get_chat()\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(response)\n",
    "        \n",
    "        # Add user message and assistant response to chat history\n",
    "        st.session_state.chat_messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
